# Kadins-BDE-Capstone
Kadin's Projects
#### &emsp;This project was created as the capstone to the Big Data Engineering class and doccuments the process I used to import and ingest external data through AWS. The grading criteria can be found in "Steps to a Successful Capstone."
##### &emsp;I chose to model a mock business in this project based on the fulfillment center I work at. To start I mapped out how I thought a business process would work for a fulfillment center. Using this process I determined what data I needed, and how it ought to look. This was represented in the logical and physical data models built using the toad data modeler program. From the physical data model I generated a Data Definition Language script, which I deployed to an AWS RDS instance, called bde-capstone, running MySQL. I did not choose MySQL for any particular reason though in the future I would likely choose to run an aroura serveless cluster, since this would give me access to the query editor.
#### &emsp;When generating the DDL I set the data type for the CreditCardNumber column in the Customers table as an int(16), however this should be a varchar(16) to avoid dropping any leading zeroes.
#### &emsp;Next, I generated 5 data tables and uploaded each table to bde-capstone RDS. The three master data tables, Customers, Employees, and Inventory represent the three discrete entities from which the transactional data is generated.
#### &emsp;The Customers table was created by removing unnecessary data from the sample-superstore.csv file, which was downloaded from kaggle. The column CreditCardNumber was created to represent the customer's payment information. The numbers used in this project are fake and could not function as payment options, however if this process was applied to real life credit card information eforts would be made to encrypt the data.
#### &emsp;The Employees table was programatically generated from a list of names. I created a column called WorkloadPerHour, which I had originally intended to act as a way to calculate how many rows should be generated in the Transactions table. I ended up using the WeeklyItemsPacked parameter to fill this role instead as I felt it had a more accurate representation of the number of items packed in a week. I believe when I was writing the script to generate the Employee data I confused myself and chose a number closer to daily output, rather than hourly.
#### &emsp;The Inventory table was generated by calling the the REST api fakestoreapi.com and formatting the responses into a pandas dataframe in python. I was unfamiliar with apis before starting this project and it took me a while to learn how to call them for this table. The information from fakestoreapi.com is useful for the purposes of this project, and is a good representation of the information that would displayed to the customer. If this project was applied to real life inventory, information on stock counts, pick locations, and weights would all need to be added. However, fakestoreapi.com does not allow for changes to be made to the items in the api, and since I was generating the Transactions table by calling the api rather than referencing the Inventory table, these could not be added.
#### &emsp;The two transactional data tables, Orders and Transactions, represent the interactions between the three master data tables. These tables were both generated at the same time order by order, though they represent two different levels of date. The Orders table looks at this on the order level, and represents the interaction between the Customer, the Employees, and the Inventory (through the transactions table). The information in this table is what would be needed by the inventory management software to generate the label and assign the order to a Packer. This table also serves as record keeping for administration purposes.
#### &emsp;The Transactions table however looks at the data on the item level, and represents the composition of each order in the Orders table. This is the information that the inventory management software would need to calculate the shipping cost and keep accurate stock counts. The Transactions table also contains the information that the Packer would need to correctly pick and pack this order.
#### &emsp;It would be possible to create one big table combining the Orders and Transactions table. This table would have the columns: OrderItemId, OrderDate, ShipDate, CustomerId, EmployeeId, OrderId, OrderCost, NumberOfItemsInOrder, ItemCountInOrder, ItemId, ItemCost, and would have a row for each item shipped. This means the information from the OrderDate column to the NumberOfItemsInOrder column would either need to be duplicated for each item in an order, or would have a null value for all but the first row of an order. To me this is a very big and unweildy data table. It made sense to me to instead have separate tables looking at the order from different perspectives.
#### &emsp;After my data was generated and uploaded, I created an S3 bucket called kadins-bde-capstone, and created folders for each of the data tables. Through the lake formation service I created a database called bde-capstone, in which I created a schema of each of the data tables.
#### &emsp;A kinesis firehose delivery stream was created for each of the tables from a direct-put to the S3 bucket. The firehose stream used the schemas to transform the data into a parquet format before delivering it to the S3 bucket. A boto3 script was generated for each of the tables. The script first queried the RDS, then converted the data into a JSON format for each row, finally it used the .put_records_batch() function to add the data to the correct firehose stream. A glue crawler was then created for each parquet file to allow them to be queried by Athena.
#### &emsp;I had particular difficulty with this step. The first problem I faced was that in the original version of my script, the .put_records_batch() function was throwing errors for my larger tables. I figured out that the reason for this was that the .put_records_batch() function is capped at 500 records, which meant that tables with more than 500 rows were to large. The easy fix for this was to segment my data before using the function. The next problem I had came when I tried querying the data through Athena. On my first attempt I tried ingesting the data without performing a transformation, which did work in that it moved the data from the RDS to the S3 bucket. However the data was in a JSON format and I could not query it with through Athena using a glue crawler. At this point I created new firehose streams which did transform the data into parquet form. I created the schemas in lake formation and ran the boto3 script with the new firehose streams. I was able to query the data through Athena after doing the transfomations.
#### &emsp;Once the data was ingested I began the process of creating a redshift cluster for long term storage. Through lake formation I verified that I could connect to the glue catalogue, and verified the two transactional tables. Then I created a Redshift cluster, which I named bde-capstone. In redshift I created the external schema capstone and created an external redshift table for each parquet table in my S3 bucket.
